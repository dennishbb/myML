{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for MMM...\n",
      "Data for MMM added successfully.\n",
      "Fetching data for AOS...\n",
      "Data for AOS added successfully.\n",
      "Fetching data for ABT...\n",
      "Data for ABT added successfully.\n",
      "Fetching data for ABBV...\n",
      "Data for ABBV added successfully.\n",
      "Fetching data for ACN...\n",
      "Data for ACN added successfully.\n",
      "Fetching data for ADBE...\n",
      "Data for ADBE added successfully.\n",
      "Fetching data for AMD...\n",
      "Data for AMD added successfully.\n",
      "Fetching data for AES...\n",
      "Data for AES added successfully.\n",
      "Fetching data for AFL...\n",
      "Data for AFL added successfully.\n",
      "Fetching data for A...\n",
      "Data for A added successfully.\n",
      "Fetching data for APD...\n",
      "Data for APD added successfully.\n",
      "Fetching data for ABNB...\n",
      "Data for ABNB added successfully.\n",
      "Fetching data for AKAM...\n",
      "Data for AKAM added successfully.\n",
      "Fetching data for ALB...\n",
      "Data for ALB added successfully.\n",
      "Fetching data for ARE...\n",
      "Data for ARE added successfully.\n",
      "Fetching data for ALGN...\n",
      "Data for ALGN added successfully.\n",
      "Fetching data for ALLE...\n",
      "Data for ALLE added successfully.\n",
      "Fetching data for LNT...\n",
      "Data for LNT added successfully.\n",
      "Fetching data for ALL...\n",
      "Data for ALL added successfully.\n",
      "Fetching data for GOOGL...\n",
      "Data for GOOGL added successfully.\n",
      "Fetching data for GOOG...\n",
      "Data for GOOG added successfully.\n",
      "Fetching data for MO...\n",
      "Data for MO added successfully.\n",
      "Fetching data for AMZN...\n",
      "Data for AMZN added successfully.\n",
      "Fetching data for AMCR...\n",
      "Data for AMCR added successfully.\n",
      "Fetching data for AEE...\n",
      "Data for AEE added successfully.\n",
      "Fetching data for AEP...\n",
      "Data for AEP added successfully.\n",
      "Fetching data for AXP...\n",
      "Data for AXP added successfully.\n",
      "Fetching data for AIG...\n",
      "Data for AIG added successfully.\n",
      "Fetching data for AMT...\n",
      "Data for AMT added successfully.\n",
      "Fetching data for AWK...\n",
      "Data for AWK added successfully.\n",
      "Fetching data for AMP...\n",
      "Data for AMP added successfully.\n",
      "Fetching data for AME...\n",
      "Data for AME added successfully.\n",
      "Fetching data for AMGN...\n",
      "Data for AMGN added successfully.\n",
      "Fetching data for APH...\n",
      "Data for APH added successfully.\n",
      "Fetching data for ADI...\n",
      "Data for ADI added successfully.\n",
      "Fetching data for ANSS...\n",
      "Data for ANSS added successfully.\n",
      "Fetching data for AON...\n",
      "Data for AON added successfully.\n",
      "Fetching data for APA...\n",
      "Data for APA added successfully.\n",
      "Fetching data for APO...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m table_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_stock_data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Fetch and store all stock data\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m \u001b[43mfetch_and_store_all_stock_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m, in \u001b[0;36mfetch_and_store_all_stock_data\u001b[0;34m(db_url, table_name)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching data for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Fetch historical data for the past two days\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m stock_data \u001b[38;5;241m=\u001b[39m \u001b[43myf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2d\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stock_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stock_data\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Add a column for the ticker symbol\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     stock_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ticker\n",
      "File \u001b[0;32m~/projects/qiskit_env/lib64/python3.11/site-packages/yfinance/utils.py:104\u001b[0m, in \u001b[0;36mlog_indent_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntering \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m IndentationContext():\n\u001b[0;32m--> 104\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/projects/qiskit_env/lib64/python3.11/site-packages/yfinance/multi.py:157\u001b[0m, in \u001b[0;36mdownload\u001b[0;34m(tickers, start, end, actions, threads, ignore_tz, group_by, auto_adjust, back_adjust, repair, keepna, progress, period, interval, prepost, proxy, rounding, timeout, session, multi_level_index)\u001b[0m\n\u001b[1;32m    150\u001b[0m         _download_one_threaded(ticker, period\u001b[38;5;241m=\u001b[39mperiod, interval\u001b[38;5;241m=\u001b[39minterval,\n\u001b[1;32m    151\u001b[0m                                start\u001b[38;5;241m=\u001b[39mstart, end\u001b[38;5;241m=\u001b[39mend, prepost\u001b[38;5;241m=\u001b[39mprepost,\n\u001b[1;32m    152\u001b[0m                                actions\u001b[38;5;241m=\u001b[39mactions, auto_adjust\u001b[38;5;241m=\u001b[39mauto_adjust,\n\u001b[1;32m    153\u001b[0m                                back_adjust\u001b[38;5;241m=\u001b[39mback_adjust, repair\u001b[38;5;241m=\u001b[39mrepair, keepna\u001b[38;5;241m=\u001b[39mkeepna,\n\u001b[1;32m    154\u001b[0m                                progress\u001b[38;5;241m=\u001b[39m(progress \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), proxy\u001b[38;5;241m=\u001b[39mproxy,\n\u001b[1;32m    155\u001b[0m                                rounding\u001b[38;5;241m=\u001b[39mrounding, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shared\u001b[38;5;241m.\u001b[39m_DFS) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(tickers):\n\u001b[0;32m--> 157\u001b[0m         _time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# download synchronously\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, ticker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tickers):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "def fetch_and_store_all_stock_data(db_url, table_name):\n",
    "    # Fetch tickers from an S&P 500 static list or replace this with a CSV/API call\n",
    "    sp500_tickers = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]['Symbol'].tolist()\n",
    "\n",
    "    # Define required columns\n",
    "    required_columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Ticker']\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for ticker in sp500_tickers:\n",
    "        try:\n",
    "            print(f\"Fetching data for {ticker}...\")\n",
    "            # Fetch historical data for the past two days\n",
    "            stock_data = yf.download(ticker, period=\"2d\", progress=False)\n",
    "\n",
    "            if stock_data is not None and not stock_data.empty:\n",
    "                # Add a column for the ticker symbol\n",
    "                stock_data['Ticker'] = ticker\n",
    "\n",
    "                # Ensure 'Adj Close' exists in the data\n",
    "                if 'Adj Close' not in stock_data.columns:\n",
    "                    stock_data['Adj Close'] = stock_data['Close']\n",
    "\n",
    "                # Reset index to make 'Date' a column\n",
    "                stock_data.reset_index(inplace=True)\n",
    "\n",
    "                # Add missing columns to match required schema\n",
    "                for col in required_columns:\n",
    "                    if col not in stock_data.columns:\n",
    "                        stock_data[col] = None\n",
    "\n",
    "                # Retain only required columns\n",
    "                stock_data = stock_data[required_columns]\n",
    "                all_data.append(stock_data)\n",
    "                print(f\"Data for {ticker} added successfully.\")\n",
    "            else:\n",
    "                print(f\"No data available for {ticker}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {ticker}: {e}\")\n",
    "\n",
    "    # Concatenate all data into a single DataFrame\n",
    "    if all_data:\n",
    "        try:\n",
    "            full_data = pd.concat(all_data, ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating data: {e}\")\n",
    "            return\n",
    "\n",
    "        # Check for missing required columns after concatenation\n",
    "        for col in required_columns:\n",
    "            if col not in full_data.columns:\n",
    "                full_data[col] = None\n",
    "\n",
    "        # Drop rows with NaN values in critical columns\n",
    "        try:\n",
    "            full_data.dropna(subset=['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Ticker'], inplace=True)\n",
    "        except KeyError as e:\n",
    "            print(f\"Error dropping NaN values: {e}\")\n",
    "            return\n",
    "\n",
    "        # Store all data in PostgreSQL using psycopg2\n",
    "        try:\n",
    "            print(f\"Storing all data into the database...\")\n",
    "            connection = psycopg2.connect(db_url)\n",
    "            cursor = connection.cursor()\n",
    "\n",
    "            # Create table if it doesn't exist\n",
    "            create_table_query = f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                date DATE,\n",
    "                open NUMERIC,\n",
    "                high NUMERIC,\n",
    "                low NUMERIC,\n",
    "                close NUMERIC,\n",
    "                adj_close NUMERIC,\n",
    "                volume BIGINT,\n",
    "                ticker VARCHAR(10)\n",
    "            )\n",
    "            \"\"\"\n",
    "            cursor.execute(create_table_query)\n",
    "\n",
    "            # Prepare data for insertion\n",
    "            records = full_data[['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Ticker']].values.tolist()\n",
    "\n",
    "            # Insert data into the table\n",
    "            insert_query = f\"\"\"\n",
    "            INSERT INTO {table_name} (date, open, high, low, close, adj_close, volume, ticker)\n",
    "            VALUES %s\n",
    "            \"\"\"\n",
    "            execute_values(cursor, insert_query, records)\n",
    "\n",
    "            # Commit and close connection\n",
    "            connection.commit()\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "            print(f\"All stock data stored successfully in {table_name} table.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error storing data to PostgreSQL: {e}\")\n",
    "    else:\n",
    "        print(\"No valid data collected. Nothing to store.\")\n",
    "\n",
    "# Database connection URL (update with your credentials)\n",
    "db_url = \"postgresql://user:password@192.168.0.34:5432/rag_db\"\n",
    "table_name = \"all_stock_data\"\n",
    "\n",
    "# Fetch and store all stock data\n",
    "fetch_and_store_all_stock_data(db_url, table_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qiskit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
